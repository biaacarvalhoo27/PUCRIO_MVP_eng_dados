{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2187893f-ce52-42b5-ab18-ea84deb60b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MVP Engenharia de Dados\n",
    "\n",
    "**Nome:** Bianca Carvalho Lima\n",
    "\n",
    "**Matrícula:** 4052025000297\n",
    "\n",
    "- **Dataset Original (de 2007 a 2020):** https://www.kaggle.com/datasets/equeiroz/acidentes-rodovias-federais-brasil-jan07-a-jul19\n",
    "- **Dataset Original (de 2023 a 2025):** https://www.kaggle.com/datasets/jairsouza/acidentes-rodovias-federais\n",
    "\n",
    "**Repo GitHub:** https://github.com/biaacarvalhoo27/PUCRIO_MVP_eng_dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e22ac00-9e3f-4c49-a688-baaa1a49f140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Coleta dos dados\n",
    "Análise de dados sobre os acidentes de trânsito nas rodovias federais brasileiras durante o período de tempo de Janeiro de 2007 até Janeiro de 2025. Os datasets foram disponibilizados pelo Plano de Dados Abertos da PRF através do site: https://www.gov.br/prf/pt-br/acesso-a-informacao/dados-abertos/dados-abertos-da-prf na categoria de Agrupados por ocorrência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b540b5a0-3715-43c9-89de-b692045f0da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Instalação de pacote Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49ee3f7-627d-41c7-936d-0dbb8317df64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cc089e4c-ae5b-43d6-8ffc-3a16accc862b/lib/python3.12/site-packages (0.3.13)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from kagglehub) (24.1)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.12/site-packages (from kagglehub) (6.0.2)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from kagglehub) (2.32.3)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cc089e4c-ae5b-43d6-8ffc-3a16accc862b/lib/python3.12/site-packages (from kagglehub) (4.67.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->kagglehub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->kagglehub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->kagglehub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->kagglehub) (2025.1.31)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf155ac6-b560-4ed5-82f8-42b36dbb5774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import dos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aff00ab-a961-41d8-82bd-927d9c3eb10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset importado com sucesso\n"
     ]
    }
   ],
   "source": [
    "acidentes_rodovias_federais_2007_2020_path = kagglehub.dataset_download('equeiroz/acidentes-rodovias-federais-brasil-jan07-a-jul19')\n",
    "acidentes_rodovias_federais_2021_20xx_path = kagglehub.dataset_download('jairsouza/acidentes-rodovias-federais')\n",
    "\n",
    "print('Dataset importado com sucesso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30cd6da4-28cf-4080-b25a-3280a9d9f526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a577a8-6348-4806-9849-f1e1dd53c402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3da52d0-37e7-4f32-a5ed-71cf73b3a846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Listagem datasets csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd32e06-c3a6-4591-86a3-053ce9a446a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Diretórios\n",
    "caminho_base = acidentes_rodovias_federais_2007_2020_path\n",
    "caminho_base_2 = acidentes_rodovias_federais_2021_20xx_path\n",
    "\n",
    "# Arquivos originais (2007–2018, 2020)\n",
    "arquivos = [\n",
    "    'datatran2007.csv',\n",
    "    'datatran2008.csv',\n",
    "    'datatran2009.csv',\n",
    "    'datatran2010.csv',\n",
    "    'datatran2011.csv',\n",
    "    'datatran2012.csv',\n",
    "    'datatran2013.csv',\n",
    "    'datatran2014.csv',\n",
    "    'datatran2015.csv',\n",
    "    #'datatran2016.csv', Não Usado\n",
    "    'datatran2017.csv',\n",
    "    'datatran2018.csv',\n",
    "    'datatran2020.csv'   \n",
    "]\n",
    "\n",
    "# Novos arquivos (2023, 2024, 2025)\n",
    "arquivos_2 = [\n",
    "    'datatran2023.csv',\n",
    "    'datatran2024.csv',\n",
    "    'datatran2025.csv'\n",
    "]\n",
    "\n",
    "dataframes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be21af92-9dbb-4851-b43f-066b4609fdee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processamento dos dados - Processo de coleta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2364929-b673-46aa-8cf9-d841df5671eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-cc089e4c-ae5b-43d6-8ffc-3a/.ipykernel/2550/command-6078046201924353-2156171893:6: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n  df_ano = pd.read_csv(caminho_completo, delimiter=';', encoding='latin1', on_bad_lines='skip')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datatran2007.csv: 127675 linhas carregadas\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-cc089e4c-ae5b-43d6-8ffc-3a/.ipykernel/2550/command-6078046201924353-2156171893:6: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n  df_ano = pd.read_csv(caminho_completo, delimiter=';', encoding='latin1', on_bad_lines='skip')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datatran2008.csv: 141043 linhas carregadas\ndatatran2009.csv: 158646 linhas carregadas\ndatatran2010.csv: 183469 linhas carregadas\ndatatran2011.csv: 192326 linhas carregadas\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-cc089e4c-ae5b-43d6-8ffc-3a/.ipykernel/2550/command-6078046201924353-2156171893:6: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n  df_ano = pd.read_csv(caminho_completo, delimiter=';', encoding='latin1', on_bad_lines='skip')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datatran2012.csv: 184568 linhas carregadas\ndatatran2013.csv: 186748 linhas carregadas\ndatatran2014.csv: 169201 linhas carregadas\ndatatran2015.csv: 122161 linhas carregadas\ndatatran2017.csv: 89518 linhas carregadas\ndatatran2018.csv: 69206 linhas carregadas\ndatatran2020.csv: 63530 linhas carregadas\ndatatran2023.csv: 67766 linhas carregadas\ndatatran2024.csv: 73156 linhas carregadas\ndatatran2025.csv: 5478 linhas carregadas\n\n Dataset consolidado: 1834491 registros\n   Período: 2007-01-01 00:00:00 a 2025-01-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Processar arquivos do primeiro diretório\n",
    "for arquivo in arquivos:\n",
    "    caminho_completo = os.path.join(caminho_base, arquivo)\n",
    "    \n",
    "    try:\n",
    "        df_ano = pd.read_csv(caminho_completo, delimiter=';', encoding='latin1', on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler {arquivo}: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{arquivo}: {len(df_ano)} linhas carregadas\")\n",
    "\n",
    "    # Extrair ano do nome do arquivo\n",
    "    ano = int(arquivo.replace('datatran', '').replace('.csv', ''))\n",
    "\n",
    "    # Converter data com base no ano\n",
    "    df_ano['data'] = pd.NaT\n",
    "    if ano <= 2011:\n",
    "        df_ano['data'] = pd.to_datetime(df_ano['data_inversa'], format='%d/%m/%Y', errors='coerce')\n",
    "    else:\n",
    "        df_ano['data'] = pd.to_datetime(df_ano['data_inversa'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "    falhas = df_ano['data'].isna().sum()\n",
    "    if falhas > 0:\n",
    "        print(f\"   {falhas} datas inválidas em {arquivo} (ano {ano})\")\n",
    "\n",
    "    dataframes.append(df_ano)\n",
    "\n",
    "# Processar arquivos do segundo diretório (2023, 2024, 2025)\n",
    "for arquivo in arquivos_2:\n",
    "    caminho_completo = os.path.join(caminho_base_2, arquivo)\n",
    "    \n",
    "    try:\n",
    "        df_ano = pd.read_csv(caminho_completo, delimiter=';', encoding='latin1', on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler {arquivo}: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{arquivo}: {len(df_ano)} linhas carregadas\")\n",
    "\n",
    "    # Extrair ano\n",
    "    ano = int(arquivo.replace('datatran', '').replace('.csv', ''))\n",
    "\n",
    "    # Todos esses são anos recentes → formato YYYY-MM-DD\n",
    "    df_ano['data'] = pd.to_datetime(df_ano['data_inversa'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "    falhas = df_ano['data'].isna().sum()\n",
    "    if falhas > 0:\n",
    "        print(f\"  {falhas} datas inválidas em {arquivo} (ano {ano})\")\n",
    "\n",
    "    dataframes.append(df_ano)\n",
    "\n",
    "# === 3. Concatenar todos os DataFrames ===\n",
    "if dataframes:\n",
    "    df = pd.concat(\n",
    "        dataframes,\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "# === 4. Resultado final ===\n",
    "    print(f\"\\n Dataset consolidado: {len(df)} registros\")\n",
    "    print(f\"   Período: {df['data'].min()} a {df['data'].max()}\")\n",
    "else:\n",
    "    print(\" Nenhum arquivo foi carregado. Verifique os caminhos e arquivos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3ca0ca-685d-4ba3-afb3-e0aec18e98e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configurar diretório schema _raw_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a89e93f-ca41-4cb7-96a8-59090ad34b87",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":301},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765047116096}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório já existe: /Volumes/lakehouse/raw/volumes/datatran\n\nEstrutura do Volume:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/Volumes/lakehouse/raw/volumes/datatran/</td><td>datatran/</td><td>0</td><td>1766201236000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/Volumes/lakehouse/raw/volumes/datatran/",
         "datatran/",
         0,
         1766201236000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "catalog_name = \"lakehouse\"\n",
    "schema_name = \"raw\"\n",
    "volume_name = \"volumes\"\n",
    "directory_name = \"datatran\"\n",
    "\n",
    "# Caminho completo do Volume\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/{directory_name}\"\n",
    "\n",
    "# Criar o diretório se não existir\n",
    "try:\n",
    "    # Verificar se o diretório existe\n",
    "    dbutils.fs.ls(volume_path)\n",
    "    print(f\"Diretório já existe: {volume_path}\")\n",
    "except:\n",
    "    # Criar diretório\n",
    "    dbutils.fs.mkdirs(volume_path)\n",
    "    print(f\"Diretório criado: {volume_path}\")\n",
    "\n",
    "# Mostrar estrutura\n",
    "print(\"\\nEstrutura do Volume:\")\n",
    "display(dbutils.fs.ls(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1c534d-9e97-4d57-9f89-72b331d005aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Camada _raw_ - dados brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa1cd69-6949-4a72-b265-31ee4cd47716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Iniciar sessão Spark\n",
    "spark = SparkSession.builder.appName(\"PRF_Data_Load\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64760ff8-b592-4c17-b3eb-eb73caa1e3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save dos datasets individuais separados por pastas com o ano correspondente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e940ba-4a33-44a4-9a56-23ccd4eb251d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Salvo: datatran2007 com 127675 registros\n Salvo: datatran2008 com 141043 registros\n Salvo: datatran2009 com 158646 registros\n Salvo: datatran2010 com 183469 registros\n Salvo: datatran2011 com 192326 registros\n Salvo: datatran2012 com 184568 registros\n Salvo: datatran2013 com 186748 registros\n Salvo: datatran2014 com 169201 registros\n Salvo: datatran2015 com 122161 registros\n Salvo: datatran2017 com 89518 registros\n Salvo: datatran2018 com 69206 registros\n Salvo: datatran2020 com 63530 registros\n Salvo: datatran2023 com 67766 registros\n Salvo: datatran2024 com 73156 registros\n Salvo: datatran2025 com 5478 registros\n"
     ]
    }
   ],
   "source": [
    "# Função para limpar dados antes da conversão\n",
    "def clean_pandas_dataframe(df_ano):\n",
    "    \"\"\"\n",
    "    Limpa o DataFrame pandas antes de converter para Spark\n",
    "    \"\"\"\n",
    "    # Colunas que precisam de tratamento especial\n",
    "    if 'km' in df_ano.columns:\n",
    "        # Converter km para numérico, tratando vírgulas\n",
    "        df_ano['km'] = df_ano['km'].astype(str).str.replace(',', '.')\n",
    "        df_ano['km'] = pd.to_numeric(df_ano['km'], errors='coerce')\n",
    "    \n",
    "    if 'br' in df_ano.columns:\n",
    "        df_ano['br'] = pd.to_numeric(df_ano['br'], errors='coerce')\n",
    "    \n",
    "    # Colunas numéricas que podem ter problemas\n",
    "    numeric_cols = ['mortos', 'feridos', 'feridos_leves', 'feridos_graves', \n",
    "                    'ilesos', 'ignorados', 'veiculos', 'pessoas']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df_ano.columns:\n",
    "            df_ano[col] = pd.to_numeric(df_ano[col], errors='coerce').fillna(0).astype('int64')\n",
    "    \n",
    "    # Garantir que strings sejam strings\n",
    "    string_cols = ['uf', 'municipio', 'causa_acidente', 'tipo_acidente']\n",
    "    for col in string_cols:\n",
    "        if col in df_ano.columns:\n",
    "            df_ano[col] = df_ano[col].astype(str)\n",
    "    \n",
    "    return df_ano\n",
    "\n",
    "for i, df_ano in enumerate(dataframes):\n",
    "    if i < len(arquivos):\n",
    "        nome_arquivo = arquivos[i].replace('.csv', '')\n",
    "    else:\n",
    "        nome_arquivo = arquivos_2[i - len(arquivos)].replace('.csv', '')\n",
    "    \n",
    "    caminho_saida = f\"/Volumes/lakehouse/raw/volumes/datatran/{nome_arquivo}\"\n",
    "    \n",
    "    # Limpar o DataFrame pandas\n",
    "    df_ano_clean = clean_pandas_dataframe(df_ano.copy())\n",
    "    \n",
    "    # Converter para Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(df_ano_clean)\n",
    "    \n",
    "    # Salvar em Parquet\n",
    "    spark_df.write.format(\"parquet\").mode(\"overwrite\").save(caminho_saida)\n",
    "    \n",
    "    print(f\" Salvo: {nome_arquivo} com {len(df_ano)} registros\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02. schema_raw",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}