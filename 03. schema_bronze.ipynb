{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6180aa9-bf77-4ee6-9796-c5fb478a5e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MVP Engenharia de Dados\n",
    "\n",
    "**Nome:** Bianca Carvalho Lima\n",
    "\n",
    "**Matrícula:** 4052025000297\n",
    "\n",
    "- **Dataset Original (de 2007 a 2020):** https://www.kaggle.com/datasets/equeiroz/acidentes-rodovias-federais-brasil-jan07-a-jul19\n",
    "- **Dataset Original (de 2023 a 2025):** https://www.kaggle.com/datasets/jairsouza/acidentes-rodovias-federais\n",
    "\n",
    "**Repo GitHub:** https://github.com/biaacarvalhoo27/PUCRIO_MVP_eng_dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47036734-cbfa-4e32-ab0c-97c56eac4037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8608e5e8-3b8f-4382-8366-16539e54b607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import lit, regexp_replace, col, to_date, try_to_date\n",
    "from pyspark.sql.types import IntegerType, DoubleType, DateType, StringType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e0ce9dd-7d98-4846-b2a0-c8e59f692d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Consolidação dos arquivos\n",
    "- Consolidação dos arquivos do schema _raw_ > \"/Volumes/lakehouse/raw/volumes/\"\n",
    "- Output da consolidação no schema _bronze_: dados padronizados, com metadados e controle de ingestão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1ccc08-c1eb-40f8-b0a4-cb22df7fcc64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSOLIDAÇÃO CONCLUÍDA!\n"
     ]
    }
   ],
   "source": [
    "def consolidar_tudo():\n",
    "    dfs = []\n",
    "    for pasta in dbutils.fs.ls(\"/Volumes/lakehouse/raw/volumes/datatran\"):\n",
    "        if pasta.isDir():\n",
    "            nome = pasta.name.rstrip(\"/\")\n",
    "            df = None\n",
    "            formato = None\n",
    "            try:\n",
    "                try:\n",
    "                    # Verifica se existe _delta_log para tentar ler como Delta\n",
    "                    delta_log_path = f\"{pasta.path}/_delta_log\"\n",
    "                    if dbutils.fs.exists(delta_log_path):\n",
    "                        df = spark.read.format(\"delta\").load(pasta.path)\n",
    "                        formato = \"Delta\"\n",
    "                    else:\n",
    "                        raise Exception(\"Delta log não encontrado\")\n",
    "                except Exception:\n",
    "                    df = spark.read.parquet(pasta.path)\n",
    "                    formato = \"Parquet\"\n",
    "\n",
    "                for coluna in df.columns:\n",
    "                    df = df.withColumn(\n",
    "                        coluna,\n",
    "                        regexp_replace(col(coluna).cast(StringType()), r\",\", \".\"),\n",
    "                    )\n",
    "                # metadados\n",
    "                df = df.withColumn(\"origem_arquivo\", lit(nome))\n",
    "                df = df.withColumn(\"formato_origem\", lit(formato))\n",
    "                df = df.withColumn(\"data_execucao\", F.current_timestamp() - F.expr(\"INTERVAL 3 HOURS\"))\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {nome}: {e}\")\n",
    "\n",
    "    if dfs:\n",
    "        df_final = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), dfs)\n",
    "        print(f\"CONSOLIDAÇÃO CONCLUÍDA!\")\n",
    "        return df_final\n",
    "    else:\n",
    "        print(\"Nenhum dado foi processado!\")\n",
    "        return None\n",
    "\n",
    "# Executar\n",
    "df_final = consolidar_tudo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e6b0a6-9256-4481-9a2e-870da38aa6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Limpeza e transformação dos dados do dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051c9405-5141-470c-9fb2-0c02910e92cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamento de tipagem das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f50c976-8798-4dbe-9128-5daca3e654e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(df_final):\n",
    "    \"\"\"\n",
    "    Função corrigida para limpeza dos dados do PRF\n",
    "    \"\"\"\n",
    "    # 1. Primeiro, tratar os valores decimais como string\n",
    "    int_cols = [\n",
    "        \"br\", \"pessoas\", \"mortos\", \"feridos_leves\", \n",
    "        \"feridos_graves\", \"ilesos\", \"ignorados\", \"feridos\", \"veiculos\"\n",
    "    ]\n",
    "    \n",
    "    for col_name in int_cols:\n",
    "        # Remover \".0\" dos valores e converter para inteiro\n",
    "        df_final = df_final.withColumn(\n",
    "            col_name,\n",
    "            F.when(\n",
    "                F.col(col_name).isNotNull(),\n",
    "                F.regexp_replace(F.col(col_name).cast(\"string\"), r\"\\.0$\", \"\")\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "        # Agora converter para inteiro\n",
    "        df_final = df_final.withColumn(\n",
    "            col_name,\n",
    "            F.col(col_name).cast(IntegerType())\n",
    "        )\n",
    "    \n",
    "    # 2. Tratar a coluna 'km' - pode ter vírgulas\n",
    "    df_final = df_final.withColumn(\n",
    "        \"km\",\n",
    "        F.when(\n",
    "            F.col(\"km\").isNotNull(),\n",
    "            F.regexp_replace(F.col(\"km\").cast(\"string\"), \",\", \".\")\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\"km\", F.col(\"km\").cast(DoubleType()))\n",
    "    \n",
    "    # 3. Corrigir ID - remover .0\n",
    "    df_final = df_final.withColumn(\n",
    "        \"id\",\n",
    "        F.regexp_replace(F.col(\"id\").cast(\"string\"), r\"\\.0$\", \"\")\n",
    "    )\n",
    "    \n",
    "    # 4. Corrigir dia_semana\n",
    "    df_final = df_final.withColumn(\n",
    "        \"dia_semana\",\n",
    "        F.when(\n",
    "            F.col(\"dia_semana\").contains(\"-\"),\n",
    "            F.initcap(F.split(F.col(\"dia_semana\"), \"-\")[0])\n",
    "        ).otherwise(F.initcap(F.col(\"dia_semana\")))\n",
    "    )\n",
    "    \n",
    "    # 5. Criar timestamp combinado\n",
    "    df_final = df_final.withColumn(\n",
    "        \"data_inversa_horario\",\n",
    "        F.to_timestamp(\n",
    "            F.concat_ws(\n",
    "                \" \",\n",
    "                F.col(\"data_inversa\").cast(\"string\"),\n",
    "                F.concat(F.col(\"horario\").cast(\"string\"), \":00\")\n",
    "            ),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b192d0-0f6f-42bd-9cec-496da16257c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamento coluna \"data_inversa\" > tipagem date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74f9813-0986-4c47-b24f-a449d7470a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_final.withColumn(\n",
    "    \"data_ajustada\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_date(data_inversa, 'yyyy-MM-dd')\"),\n",
    "        F.expr(\"try_to_date(data_inversa, 'dd/MM/yyyy')\"),\n",
    "        F.expr(\"try_to_date(data_inversa, 'dd-MM-yyyy')\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907d4a0b-fa98-4a1f-96c8-b55d4ffbb4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Remoção espaços entre palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f5d605-96c2-4b57-bf59-95cbe4b5fa81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_final.withColumn(\n",
    "    \"condicao_metereologica\",\n",
    "    F.trim(F.col(\"condicao_metereologica\"))\n",
    ").withColumn(\n",
    "    \"causa_acidente\",\n",
    "    F.trim(F.col(\"causa_acidente\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02dda9d4-e938-483d-ad97-5eff4516c32e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tratamento e padronização dados da coluna condicao_metereologica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104653f8-314e-47e7-9f2a-fd947cc914d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_final.withColumn(\n",
    "    \"condicao_metereologica\",\n",
    "    F.when(\n",
    "        F.col(\"condicao_metereologica\").isin(\"Nevoeiro/neblina\", \"Nevoeiro/Neblina\"),\n",
    "        \"Nevoeiro/Neblina\"\n",
    "    ).when(\n",
    "        F.col(\"condicao_metereologica\").isin(\"(null)\", \"Ignorado\"),\n",
    "        \"Ignorada\"\n",
    "    ).when(\n",
    "        F.col(\"condicao_metereologica\").isin(\"Ceu Claro\", \"CÃ©u Claro\"),\n",
    "        \"Céu Claro\"\n",
    "    ).otherwise(F.col(\"condicao_metereologica\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07cb446e-3d48-4a14-b3bc-27e5ff482f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Remover duplicidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da483bef-86b4-4a52-b5ea-6f8bc1162d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_final.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad78a983-01d3-4eb7-88fc-f0dc6aed0ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save da tabela em delta no schema bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e9abbd-6a09-4a47-b9f0-7ed6c8c27231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.write.option(\"inferSchema\", False).mode(\"overwrite\").option(\"encoding\", \"UTF-8\").format(\"delta\").saveAsTable(\n",
    "    \"lakehouse.bronze.datatran_consolidado\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03. schema_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}